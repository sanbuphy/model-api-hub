<div align="center">

<img src="assets/logo.png" alt="Model API Hub Logo" width="120" height="120">

# Model API Hub

**One Line of Code, Access to 100+ AI Models**

[![GitHub release](https://img.shields.io/github/v/release/username/translamate)](https://github.com/username/translamate/releases)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![GitHub Actions](https://img.shields.io/github/actions/workflow/status/username/translamate/ci.yml)](https://github.com/username/translamate/actions)
[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyPI version](https://badge.fury.io/py/model-api-hub.svg)](https://badge.fury.io/py/model-api-hub)

[English](README.md) Â· [ç®€ä½“ä¸­æ–‡](README.zh-CN.md)

</div>

---

## âœ¨ What is Model API Hub?

Model API Hub is a **unified Python SDK** that lets you access multiple AI model APIs across different platforms and modalities with a **consistent, simple interface**.

Stop juggling different SDKs for each provider. Use one library for everything.

```python
# Same interface, different providers
from model_api_hub import deepseek_chat, siliconflow_chat, kimi_chat

# All work the same way
response = deepseek_chat("Hello!")
response = siliconflow_chat("Hello!")
response = kimi_chat("Hello!")
```

---

## ğŸ¯ Key Features

| Feature | Description |
|---------|-------------|
| ğŸ”Œ **15+ Providers** | OpenAI, Anthropic, DeepSeek, ZhipuAI, Kimi, SiliconFlow, and more |
| ğŸ¨ **5 Modalities** | LLM, Vision-Language, Image Gen, Audio TTS, Video Gen |
| ğŸš€ **One-Line Setup** | `pip install model-api-hub` and you're ready |
| ğŸ”„ **Unified API** | Same interface across all providers |
| âš™ï¸ **Flexible Config** | `.env`, YAML, or direct API keys |
| ğŸ› ï¸ **CLI Included** | Test models directly from command line |
| ğŸ“¦ **Zero Dependencies** | Lightweight, only essential packages |

---

## ğŸš€ Quick Start

### Installation

```bash
pip install model-api-hub
```

### 1. Set Your API Key

```bash
# Create .env file
echo 'DEEPSEEK_API_KEY=your_key_here' > .env
```

### 2. Start Coding

```python
from model_api_hub import deepseek_chat

# That's it. You're done.
response = deepseek_chat("Explain quantum computing in simple terms")
print(response)
```

---

## ğŸ“– Usage Examples

### ğŸ¤– Language Models (LLM)

```python
from model_api_hub import deepseek_chat, kimi_chat, siliconflow_chat

# DeepSeek
response = deepseek_chat(
    "Write a Python function to sort a list",
    system_prompt="You are a coding expert."
)

# Kimi (Moonshot)
response = kimi_chat(
    "Summarize this article",
    temperature=0.5
)

# SiliconFlow
response = siliconflow_chat("Hello!", model="deepseek-ai/DeepSeek-V3")
```

### ğŸ‘ï¸ Vision-Language Models (VLM)

```python
from model_api_hub import siliconflow_analyze_image

response = siliconflow_analyze_image(
    image_path="photo.jpg",
    prompt="What's in this image?"
)
```

### ğŸ¨ Image Generation

```python
from model_api_hub import siliconflow_text_to_image

siliconflow_text_to_image(
    prompt="A serene mountain landscape at sunset",
    output_path="landscape.png",
    image_size="1024x1024"
)
```

### ğŸ”Š Text-to-Speech

```python
from model_api_hub import elevenlabs_tts

elevenlabs_tts(
    text="Hello, this is a test.",
    output_path="output.mp3",
    voice_id="21m00Tcm4TlvDq8ikWAM"
)
```

### ğŸ¬ Video Generation

```python
from model_api_hub import runway_generate_video

runway_generate_video(
    prompt="A drone flying over a tropical forest",
    output_path="video.mp4",
    duration=5
)
```

---

## ğŸ› ï¸ Command Line Interface

```bash
# List all available providers
model-api-hub ls

# Quick test with DeepSeek
model-api-hub deepseek "Hello, how are you?"

# Generate an image
model-api-hub siliconflow-image "A beautiful sunset" --output sunset.png

# Analyze an image
model-api-hub siliconflow-vlm "Describe this image" --image photo.jpg

# Text-to-speech
model-api-hub elevenlabs-tts "Hello world" --output hello.mp3
```

---

## ğŸ¯ Supported Models

Model API Hub supports **500+ AI models** across **25+ providers** and **5 modalities**.

### ï¿½ Quick Model Reference

<table align="center">
  <tr>
    <td valign="top" width="25%">
      <b>ğŸ”¥ Hot Models</b><br>
      â€¢ <a href="./support_model.md#deepseek-r1">DeepSeek-R1</a><br>
      â€¢ <a href="./support_model.md#deepseek-v3">DeepSeek-V3</a><br>
      â€¢ <a href="./support_model.md#glm-47">GLM-4.7</a><br>
      â€¢ <a href="./support_model.md#glm-45">GLM-4.5</a><br>
      â€¢ <a href="./support_model.md#qwen3">Qwen3</a><br>
      â€¢ <a href="./support_model.md#qwen25">Qwen2.5</a><br>
      â€¢ <a href="./support_model.md#kimi-k25">Kimi K2.5</a><br>
      â€¢ <a href="./support_model.md#minimax-m2">MiniMax-M2</a><br>
      â€¢ <a href="./support_model.md#ernie-45">ERNIE-4.5</a><br>
      â€¢ <a href="./support_model.md#doubao-pro">Doubao-Pro</a><br>
      â€¢ <a href="./support_model.md#llama-31">Llama-3.1</a><br>
      â€¢ <a href="./support_model.md#gpt-4o">GPT-4o</a><br>
      â€¢ <a href="./support_model.md#claude-35-sonnet">Claude-3.5-Sonnet</a>
    </td>
    <td valign="top" width="25%">
      <b>ğŸ‡¨ğŸ‡³ Domestic LLM</b><br>
      â€¢ <a href="./support_model.md#deepseek">DeepSeek</a><br>
      â€¢ <a href="./support_model.md#glm">GLM/æ™ºè°±AI</a><br>
      â€¢ <a href="./support_model.md#qwen">Qwen/é€šä¹‰åƒé—®</a><br>
      â€¢ <a href="./support_model.md#kimi">Kimi/æœˆä¹‹æš—é¢</a><br>
      â€¢ <a href="./support_model.md#ernie">ERNIE/æ–‡å¿ƒä¸€è¨€</a><br>
      â€¢ <a href="./support_model.md#minimax">MiniMax</a><br>
      â€¢ <a href="./support_model.md#spark">Spark/è®¯é£æ˜Ÿç«</a><br>
      â€¢ <a href="./support_model.md#doubao">Doubao/è±†åŒ…</a><br>
      â€¢ <a href="./support_model.md#baichuan">Baichuan/ç™¾å·</a><br>
      â€¢ <a href="./support_model.md#yi">Yi/é›¶ä¸€ä¸‡ç‰©</a><br>
      â€¢ <a href="./support_model.md#hunyuan">Hunyuan/è…¾è®¯æ··å…ƒ</a><br>
      â€¢ <a href="./support_model.md#sensechat">SenseChat/å•†æ±¤</a>
    </td>
    <td valign="top" width="25%">
      <b>ğŸŒ International LLM</b><br>
      â€¢ <a href="./support_model.md#gpt">GPT/OpenAI</a><br>
      â€¢ <a href="./support_model.md#claude">Claude/Anthropic</a><br>
      â€¢ <a href="./support_model.md#gemini">Gemini/Google</a><br>
      â€¢ <a href="./support_model.md#llama">Llama/Meta</a><br>
      â€¢ <a href="./support_model.md#mistral">Mistral AI</a><br>
      â€¢ <a href="./support_model.md#cohere">Cohere</a><br>
      â€¢ <a href="./support_model.md#grok">Grok/xAI</a><br>
      â€¢ <a href="./support_model.md#ai21">AI21 Labs</a><br>
      â€¢ <a href="./support_model.md#perplexity">Perplexity</a><br>
      â€¢ <a href="./support_model.md#together">Together AI</a><br>
      â€¢ <a href="./support_model.md#groq">Groq</a><br>
      â€¢ <a href="./support_model.md#fireworks">Fireworks</a>
    </td>
    <td valign="top" width="25%">
      <b>ğŸ¨ Multimodal</b><br>
      â€¢ <a href="./support_model.md#qwen-vl">Qwen-VL</a><br>
      â€¢ <a href="./support_model.md#glm-4v">GLM-4V</a><br>
      â€¢ <a href="./support_model.md#gpt-4v">GPT-4V</a><br>
      â€¢ <a href="./support_model.md#gemini-vision">Gemini Vision</a><br>
      â€¢ <a href="./support_model.md#claude-vision">Claude Vision</a><br>
      â€¢ <a href="./support_model.md#dall-e">DALL-E 3</a><br>
      â€¢ <a href="./support_model.md#stable-diffusion">Stable Diffusion</a><br>
      â€¢ <a href="./support_model.md#midjourney">Midjourney</a><br>
      â€¢ <a href="./support_model.md#whisper">Whisper</a><br>
      â€¢ <a href="./support_model.md#elevenlabs">ElevenLabs</a><br>
      â€¢ <a href="./support_model.md#runway">Runway</a><br>
      â€¢ <a href="./support_model.md#luma">Luma AI</a>
    </td>
  </tr>
</table>

### ğŸ¢ API Aggregators (ä¸­è½¬ç«™)

<table align="center">
  <tr>
    <td valign="top" width="50%">
      <b>ğŸ‡¨ğŸ‡³ Domestic Aggregators</b><br>
      â€¢ <a href="./support_model.md#qiniu-ai">ä¸ƒç‰›äº‘ AI</a> - ä¸­å›½ç‰ˆ OpenRouter<br>
      â€¢ <a href="./support_model.md#ppio">PPIO æ´¾æ¬§äº‘</a> - ä¸€ç«™å¼ AI äº‘æœåŠ¡<br>
      â€¢ <a href="./support_model.md#coreshub">åŸºçŸ³æ™ºç®—</a> - é’äº‘ç§‘æŠ€ AI ç®—åŠ›<br>
      â€¢ <a href="./support_model.md#ucloud">UCloud ä¼˜åˆ»å¾—</a> - å­”æ˜æ™ºç®—å¹³å°<br>
      â€¢ <a href="./support_model.md#kuaishou">å¿«æ‰‹ä¸‡æ“</a> - KAT-Coder ç¼–ç¨‹æ¨¡å‹<br>
      â€¢ <a href="./support_model.md#ksyun">é‡‘å±±äº‘æ˜Ÿæµ</a> - AI è®­æ¨å…¨æµç¨‹<br>
      â€¢ <a href="./support_model.md#infinigence">æ— é—®èŠ¯ç©¹</a> - å¼‚æ„ç®—åŠ›é›†ç¾¤<br>
      â€¢ <a href="./support_model.md#lanyun">è“è€˜å…ƒç”Ÿä»£</a> - é«˜æ€§èƒ½æ¨ç†<br>
      â€¢ <a href="./support_model.md#gitee">æ¨¡åŠ›æ–¹èˆŸ</a> - Gitee AI å¹¿åœº<br>
      â€¢ <a href="./support_model.md#paratera">å¹¶è¡Œæ™ºç®—äº‘</a> - æ¨¡å‹å¹¿åœº<br>
      â€¢ <a href="./support_model.md#volcengine">ç«å±±æ–¹èˆŸ</a> - å­—èŠ‚ MaaS<br>
      â€¢ <a href="./support_model.md#sophnet">SophNet</a> - ç®—èƒ½ç§‘æŠ€<br>
      â€¢ <a href="./support_model.md#siliconflow">SiliconFlow</a> - 50+ å¼€æºæ¨¡å‹<br>
      â€¢ <a href="./support_model.md#ai302">302.AI</a> - ä¸€ç«™å¼ AI æœåŠ¡
    </td>
    <td valign="top" width="50%">
      <b>ğŸŒ International Aggregators</b><br>
      â€¢ <a href="./support_model.md#openrouter">OpenRouter</a> - 200+ æ¨¡å‹ç»Ÿä¸€è®¿é—®<br>
      â€¢ <a href="./support_model.md#poe">Poe</a> - å¤šæ¨¡å‹èšåˆå¹³å°<br>
      â€¢ <a href="./support_model.md#groq">Groq</a> - æé€Ÿæ¨ç†å¼•æ“<br>
      â€¢ <a href="./support_model.md#together">Together AI</a> - å¼€æºæ¨¡å‹å¹³å°<br>
      â€¢ <a href="./support_model.md#fireworks">Fireworks</a> - å¿«é€Ÿæ¨ç†æœåŠ¡<br>
      â€¢ <a href="./support_model.md#novita">Novita AI</a> - å¼€æºæ¨¡å‹ API<br>
      â€¢ <a href="./support_model.md#anyscale">Anyscale</a> - ç”Ÿäº§çº§éƒ¨ç½²<br>
      â€¢ <a href="./support_model.md#perplexity">Perplexity</a> - æœç´¢å¢å¼º LLM<br>
      â€¢ <a href="./support_model.md#mistral">Mistral AI</a> - æ¬§æ´²é¢†å…ˆæ¨¡å‹<br>
      â€¢ <a href="./support_model.md#cohere">Cohere</a> - ä¼ä¸šçº§ NLP<br>
      â€¢ <a href="./support_model.md#ai21">AI21 Labs</a> - Jurassic æ¨¡å‹
    </td>
  </tr>
</table>

ğŸ“– **Full model documentation**: [support_model.md](./support_model.md)

---

## ï¿½ğŸ“‹ Supported Providers by Modality

### Language Models

| Provider | Import | Models |
|----------|--------|--------|
| **DeepSeek** | `deepseek_chat` | deepseek-chat, deepseek-reasoner |
| **SiliconFlow** | `siliconflow_chat` | DeepSeek-V3, GLM-4.5, Kimi-K2, Qwen3 |
| **Kimi** | `kimi_chat` | moonshot-v1-128k, moonshot-v1-32k |
| **ZhipuAI** | `zhipuai_chat` | glm-4-plus, glm-4-air, glm-4-flash |
| **Yiyan** | `yiyan_chat` | ernie-4.0-8k, ernie-3.5-8k |
| **MiniMax** | `minimax_chat` | abab6.5s-chat, abab6.5-chat |

### Vision-Language Models

| Provider | Import | Models |
|----------|--------|--------|
| **SiliconFlow** | `siliconflow_analyze_image` | Qwen3-VL, GLM-4.5V, step3 |
| **Yiyan** | `yiyan_analyze_image` | ernie-vision-4.0 |

### Image Generation

| Provider | Import | Models |
|----------|--------|--------|
| **SiliconFlow** | `siliconflow_text_to_image` | Kolors, FLUX.1, SD3 |
| **Recraft** | `recraft_text_to_image` | recraft-v3 |

### Audio (TTS)

| Provider | Import | Models |
|----------|--------|--------|
| **ElevenLabs** | `elevenlabs_tts` | eleven_multilingual_v2 |
| **OpenAI** | `openai_tts` | tts-1, tts-1-hd |

### Video Generation

| Provider | Import | Models |
|----------|--------|--------|
| **Runway** | `runway_generate_video` | gen3a_turbo |
| **Luma** | `luma_generate_video` | genie-1.0 |

---

## âš™ï¸ Configuration

### Option 1: Environment Variables (Recommended)

Create a `.env` file in your project root:

```bash
DEEPSEEK_API_KEY=your_key_here
KIMI_API_KEY=your_key_here
SILICONFLOW_API_KEY=your_key_here
# ... add more as needed
```

The package will automatically load these variables using `python-dotenv`.

### Option 2: Direct Import & Modify

You can directly import the module and modify the API key in code:

```python
from model_api_hub.api.llm import deepseek_llm

# Modify the API key directly
deepseek_llm.API_KEY = "your_api_key_here"

# Now use the functions
response = deepseek_llm.chat("Hello!")
```

Or import specific functions and pass API key as parameter:

```python
from model_api_hub.api.llm.deepseek_llm import chat

response = chat("Hello!", api_key="your_key_here")
```

### Option 3: YAML Config

Create `config.yaml`:

```yaml
llm:
  deepseek:
    model: "deepseek-chat"
    temperature: 0.7
    max_tokens: 4096

vlm:
  siliconflow:
    model: "Qwen/Qwen3-VL-8B-Instruct"
```

Then load it in your code:

```python
from model_api_hub.utils.config import ConfigManager

config = ConfigManager()
api_key = config.get_api_key("deepseek")
```

---

## ğŸ§ª Testing

All test files support direct execution with `if __name__ == "__main__"`:

```bash
# Test LLM providers
python tests/test_llm.py

# Test VLM providers (requires test image)
python tests/test_vlm.py

# Test Image Generation
python tests/test_image.py

# Test Audio (TTS)
python tests/test_audio.py

# Test Video Generation (takes time)
python tests/test_video.py
```

Before running tests, set your API key in the `.env` file or modify it directly in the test file.

---

## ğŸ¤ Contributing

We welcome contributions! Here's how to add a new provider:

1. **Fork** the repository
2. Create a new file in `model_api_hub/api/{category}/`
3. Follow the naming convention: `{provider}_{category}.py`
4. Implement standard functions: `create_client()`, `chat()` or `generate_*()`
5. Add CLI support in `model_api_hub/cli.py`
6. Update `model_api_hub/__init__.py` exports
7. Add tests in `tests/`
8. Submit a **Pull Request**

See [CONTRIBUTING.md](CONTRIBUTING.md) for detailed guidelines.

---

## ğŸ“Š Project Stats

<div align="center">

[![Star History Chart](https://api.star-history.com/svg?repos=username/translamate&type=Date)](https://star-history.com/#username/translamate&Date)

</div>

---

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

Thanks to all the AI providers for their amazing APIs:

[DeepSeek](https://www.deepseek.com/) â€¢ [Kimi](https://www.moonshot.cn/) â€¢ [ZhipuAI](https://open.bigmodel.cn/) â€¢ [SiliconFlow](https://cloud.siliconflow.cn/) â€¢ [OpenRouter](https://openrouter.ai/) â€¢ [MiniMax](https://www.minimaxi.com/) â€¢ [Runway](https://runwayml.com/) â€¢ [Luma AI](https://lumalabs.ai/) â€¢ [ElevenLabs](https://elevenlabs.io/)

---

<div align="center">

**â­ Star us on GitHub â€” it motivates us a lot!**

[Report Bug](https://github.com/username/translamate/issues) â€¢ [Request Feature](https://github.com/username/translamate/issues) â€¢ [Documentation](https://github.com/username/translamate/wiki)

</div>
