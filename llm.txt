# Model API Hub - LLM Module Documentation for AI Assistants

## Project Overview

Model API Hub is a unified Python SDK for accessing 100+ AI models across different providers and modalities (LLM, VLM, Image Gen, Audio TTS, Video Gen).

## Quick Start for AI Assistants

When working with this codebase, here are the key patterns:

### 1. Basic LLM Usage Pattern

```python
# All LLM providers follow the same interface
from model_api_hub.api.llm.deepseek_llm import chat as deepseek_chat
from model_api_hub.api.llm.openai_llm import chat as openai_chat
from model_api_hub.api.llm.siliconflow_llm import chat as siliconflow_chat

# All use the same signature
response = deepseek_chat(
    prompt="Your question here",
    system_prompt="Optional system prompt",
    api_key="your_api_key",  # or loads from .env
    model="model_name",      # optional
    temperature=0.7,         # optional
    max_tokens=4096          # optional
)
```

### 2. Available LLM Providers

All providers are in `model_api_hub/api/llm/`:

| Provider | Module | Chat Function | Stream Support |
|----------|--------|---------------|----------------|
| OpenAI | openai_llm | openai_chat | ✅ Yes |
| Anthropic Claude | anthropic_llm | anthropic_chat | ✅ Yes |
| DeepSeek | deepseek_llm | deepseek_chat | ✅ Yes |
| Google Gemini | gemini_llm | gemini_chat | ✅ Yes |
| Kimi (Moonshot) | kimi_llm | kimi_chat | ✅ Yes |
| ZhipuAI (智谱) | zhipuai_llm | zhipuai_chat | ✅ Yes |
| SiliconFlow | siliconflow_llm | siliconflow_chat | ✅ Yes |
| MiniMax | minimax_llm | minimax_chat | ✅ Yes |
| Baidu Yiyan | yiyan_llm | yiyan_chat | ✅ Yes |
| Alibaba DashScope | dashscope_llm | dashscope_chat | ✅ Yes |
| ModelScope | modelscope_llm | modelscope_chat | ✅ Yes |
| Xunfei Spark | xunfei_llm | xunfei_chat | ✅ Yes |
| Groq | groq_llm | groq_chat | ✅ Yes |
| Together AI | together_llm | together_chat | ✅ Yes |
| Mistral | mistral_llm | mistral_chat | ✅ Yes |
| Cohere | cohere_llm | cohere_chat | ✅ Yes |
| Perplexity | perplexity_llm | perplexity_chat | ✅ Yes |
| Azure OpenAI | azure_openai_llm | azure_chat | ✅ Yes |

### 3. Synchronous Chat API

```python
from model_api_hub.api.llm.deepseek_llm import chat

# Simple usage
response = chat("Hello, how are you?")

# With all parameters
response = chat(
    prompt="Explain quantum computing",
    system_prompt="You are a physics professor",
    api_key="sk-...",           # Uses env var if not provided
    model="deepseek-chat",       # Provider-specific model name
    temperature=0.7,
    max_tokens=2048,
    top_p=0.9
)
```

### 4. Streaming Chat API

```python
from model_api_hub.api.llm.deepseek_llm import chat_stream

# Stream usage - yields chunks as they arrive
for chunk in chat_stream("Tell me a long story"):
    print(chunk, end="", flush=True)

# With all parameters
for chunk in chat_stream(
    prompt="Write a poem",
    system_prompt="You are a poet",
    api_key="sk-...",
    model="deepseek-chat",
    temperature=0.8
):
    print(chunk, end="", flush=True)
```

### 5. Low-Level API (Advanced)

```python
from model_api_hub.api.llm.deepseek_llm import create_client, get_completion

# Create client
client = create_client(api_key="sk-...")

# Build messages
messages = [
    {"role": "system", "content": "You are helpful"},
    {"role": "user", "content": "Hello!"},
    {"role": "assistant", "content": "Hi there!"},
    {"role": "user", "content": "How are you?"}
]

# Get completion
response = get_completion(
    client=client,
    messages=messages,
    model="deepseek-chat",
    temperature=0.7,
    stream=False
)
```

### 6. Environment Configuration

Create `.env` file in project root:

```bash
# LLM Providers
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-...
DEEPSEEK_API_KEY=sk-...
KIMI_API_KEY=sk-...
ZHIPUAI_API_KEY=...
SILICONFLOW_API_KEY=sk-...
MINIMAX_API_KEY=...
YIYAN_API_KEY=...
DASHSCOPE_API_KEY=sk-...
MODELSCOPE_API_KEY=ms-...
XUNFEI_SPARK_API_KEY=...
GROQ_API_KEY=gsk_...
TOGETHER_API_KEY=...
MISTRAL_API_KEY=...
COHERE_API_KEY=...
PERPLEXITY_API_KEY=pplx-...
AZURE_OPENAI_API_KEY=...
```

### 7. Common Parameters

All `chat()` and `chat_stream()` functions support:

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| prompt | str | required | User message |
| system_prompt | str | None | System instruction |
| api_key | str | None | API key (loads from env if None) |
| model | str | provider default | Model name |
| temperature | float | 0.7 | Randomness (0-2) |
| max_tokens | int | 4096 | Max response length |
| top_p | float | 0.9 | Nucleus sampling |

### 8. Testing

Run tests from project root:

```bash
# Test all LLMs
python tests/test_llm.py

# Test specific provider
python tests/test_llm_streaming.py
```

### 9. Adding a New LLM Provider

To add a new provider, create a file in `model_api_hub/api/llm/`:

```python
"""
NewProvider LLM API wrapper.
"""
from openai import OpenAI  # or appropriate client
from typing import Dict, Any, List, Optional
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent.parent))
from model_api_hub.utils.config import get_api_key

DEFAULT_BASE_URL = "https://api.newprovider.com/v1"
DEFAULT_MODEL = "newprovider-model"


def create_client(api_key: Optional[str] = None, base_url: str = DEFAULT_BASE_URL) -> OpenAI:
    """Create API client."""
    if api_key is None:
        api_key = get_api_key("newprovider")
    return OpenAI(api_key=api_key, base_url=base_url)


def get_completion(
    client: OpenAI,
    messages: List[Dict[str, Any]],
    model: str = DEFAULT_MODEL,
    max_tokens: int = 4096,
    temperature: float = 0.7,
    stream: bool = False,
    **kwargs
) -> str:
    """Get completion from API."""
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        max_tokens=max_tokens,
        temperature=temperature,
        stream=stream,
        **kwargs
    )
    if stream:
        return response  # Return iterator for streaming
    return response.choices[0].message.content


def chat(
    prompt: str,
    system_prompt: Optional[str] = None,
    api_key: Optional[str] = None,
    model: str = DEFAULT_MODEL,
    **kwargs
) -> str:
    """Synchronous chat."""
    client = create_client(api_key=api_key)
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})
    return get_completion(client, messages, model=model, stream=False, **kwargs)


def chat_stream(
    prompt: str,
    system_prompt: Optional[str] = None,
    api_key: Optional[str] = None,
    model: str = DEFAULT_MODEL,
    **kwargs
):
    """Streaming chat - yields text chunks."""
    client = create_client(api_key=api_key)
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})
    
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        stream=True,
        **kwargs
    )
    
    for chunk in response:
        if chunk.choices and chunk.choices[0].delta.content:
            yield chunk.choices[0].delta.content
```

Then add to `model_api_hub/api/llm/__init__.py`:

```python
from .newprovider_llm import chat as newprovider_chat, create_client as newprovider_client

__all__ = [
    # ... existing exports ...
    "newprovider_chat",
    "newprovider_client",
]
```

### 10. File Structure

```
model_api_hub/
├── api/
│   ├── llm/                    # LLM providers
│   │   ├── __init__.py
│   │   ├── openai_llm.py
│   │   ├── deepseek_llm.py
│   │   ├── kimi_llm.py
│   │   └── ... (other providers)
│   ├── vlm/                    # Vision-Language models
│   ├── image/                  # Image generation
│   ├── audio/                  # Text-to-Speech
│   └── video/                  # Video generation
├── utils/
│   ├── config.py               # Configuration management
│   └── __init__.py
├── __init__.py
└── cli.py                      # Command-line interface

tests/
├── test_llm.py                 # Synchronous LLM tests
├── test_llm_streaming.py       # Streaming LLM tests
├── test_vlm.py
├── test_image.py
├── test_audio.py
└── test_video.py
```

### 11. Key Design Principles

1. **Unified Interface**: All providers use the same `chat()` and `chat_stream()` signatures
2. **Environment First**: API keys load from `.env` automatically
3. **Streaming Support**: All providers support both sync and streaming modes
4. **Type Safety**: Full type hints for all functions
5. **Error Handling**: Graceful handling of missing keys and API errors
6. **Extensibility**: Easy to add new providers following the pattern

### 12. Common Issues & Solutions

**Issue**: API key not found
```python
# Solution: Check .env file or pass explicitly
response = chat("Hello", api_key="sk-...")
```

**Issue**: Model not found
```python
# Solution: Check provider's available models
response = chat("Hello", model="correct-model-name")
```

**Issue**: Streaming not working
```python
# Solution: Use chat_stream() instead of chat()
for chunk in chat_stream("Hello"):
    print(chunk, end="")
```

## Notes for AI Assistants

- Always check if a provider is already implemented before adding new ones
- Follow the existing naming conventions: `{provider}_llm.py` for modules
- Use `create_client()` pattern for consistency
- Support both sync (`chat`) and async/streaming (`chat_stream`) modes
- Add tests in `tests/test_llm.py` and `tests/test_llm_streaming.py`
- Update `model_api_hub/api/llm/__init__.py` exports
- Document the provider in this file
